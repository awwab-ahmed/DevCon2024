{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn \n",
    "import torch.optim \n",
    "import torchvision.transforms \n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import ImageFolder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path_train = r'C:/Users/awwab/OneDrive/Desktop/DevCon 2024/images/train'\n",
    "image_path_test = r'C:/Users/awwab/OneDrive/Desktop/DevCon 2024/images/test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define a simple CNN\n",
    "class CNN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        #first convolutional layer\n",
    "        self.conv1 = torch.nn.Conv2d(3, 32, kernel_size=5, stride=1, padding=2)\n",
    "        self.relu1 = torch.nn.ReLU()\n",
    "        #max pooling layer\n",
    "        self.pool1 = torch.nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        #second convolutional layer\n",
    "        self.conv2 = torch.nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu2 = torch.nn.ReLU()\n",
    "        #max pooling layer\n",
    "        self.pool2 = torch.nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        #first fully connected layer\n",
    "        self.fc1 = torch.nn.Linear(64 * 56 * 56, 128)\n",
    "        self.relu3 = torch.nn.ReLU()\n",
    "        #second fully connected layer\n",
    "        self.fc2 = torch.nn.Linear(128, 2)  \n",
    "\n",
    "    #defines a forward pass of the model\n",
    "    def forward(self, x):\n",
    "        x = self.pool1(self.relu1(self.conv1(x)))\n",
    "        x = self.pool2(self.relu2(self.conv2(x)))\n",
    "        x = x.view(-1, 64 * 56 * 56)\n",
    "        x = self.relu3(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#resizes each image to 224x224, converts to tensor, and normalizes \n",
    "transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Resize((224, 224)),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    #numbers below are taken from the pre-computed values of the ImageNet dataset\n",
    "    torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load datasets\n",
    "train_dataset = ImageFolder(root=image_path_train, transform=transform)\n",
    "test_dataset = ImageFolder(root=image_path_test, transform=transform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "#creating the data loaders\n",
    "#batch and shuffle the data during training\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#instant of the CNN class\n",
    "model = CNN()\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "#optimizing the model parameters\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNN(\n",
       "  (conv1): Conv2d(3, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "  (relu1): ReLU()\n",
       "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (relu2): ReLU()\n",
       "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (fc1): Linear(in_features=200704, out_features=128, bias=True)\n",
       "  (relu3): ReLU()\n",
       "  (fc2): Linear(in_features=128, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#setup for training\n",
    "num_epochs = 8\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8, Train Loss: 2.5567, Test Loss: 0.6183, Test Accuracy: 0.7750\n",
      "Epoch 2/8, Train Loss: 0.6655, Test Loss: 0.5992, Test Accuracy: 0.7250\n",
      "Epoch 3/8, Train Loss: 0.5424, Test Loss: 0.4358, Test Accuracy: 0.8750\n",
      "Epoch 4/8, Train Loss: 0.3610, Test Loss: 0.3099, Test Accuracy: 0.8250\n",
      "Epoch 5/8, Train Loss: 0.2386, Test Loss: 0.1346, Test Accuracy: 0.9500\n",
      "Epoch 6/8, Train Loss: 0.2068, Test Loss: 0.2163, Test Accuracy: 0.8750\n",
      "Epoch 7/8, Train Loss: 0.1575, Test Loss: 0.1303, Test Accuracy: 0.9750\n",
      "Epoch 8/8, Train Loss: 0.1244, Test Loss: 0.2386, Test Accuracy: 0.8750\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * images.size(0)\n",
    "\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    test_corrects = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            test_loss += loss.item() * images.size(0)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            test_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "    train_loss = train_loss / len(train_loader.dataset)\n",
    "    test_loss = test_loss / len(test_loader.dataset)\n",
    "    test_accuracy = test_corrects.double() / len(test_loader.dataset)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load example images for example testing\n",
    "from PIL import Image\n",
    "image_good = Image.open(r'C:/Users/awwab/OneDrive/Desktop/DevCon 2024/images/example_good_condition.jpg')\n",
    "image_bad = Image.open(r'C:/Users/awwab/OneDrive/Desktop/DevCon 2024/images/example_bad_condition.jpg')\n",
    "#prepare images into image tensors\n",
    "image_tensor_good = transform(image_good).unsqueeze(0).to(device)\n",
    "image_tensor_bad = transform(image_bad).unsqueeze(0).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The image is classified as a terrestrial ecosystem that is in good condition.\n"
     ]
    }
   ],
   "source": [
    "#evaluation mode\n",
    "model.eval()\n",
    "\n",
    "#example using an image of a terrestrial ecosystem in good condition \n",
    "with torch.no_grad():\n",
    "    outputs = model(image_tensor_good)\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "#list of labels that are used to predict\n",
    "result = ['bad condition', 'good condition']\n",
    "prediction = result[predicted.item()]\n",
    "\n",
    "print(f\"The image is classified as a terrestrial ecosystem that is in {prediction}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The image is classified as a terrestrial ecosystem that is in bad condition.\n"
     ]
    }
   ],
   "source": [
    "##example using an image of a terrestrial ecosystem in bad condition \n",
    "with torch.no_grad():\n",
    "    outputs = model(image_tensor_bad)\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "#list of labels that are used to predict\n",
    "result = ['bad condition', 'good condition']\n",
    "prediction = result[predicted.item()]\n",
    "\n",
    "print(f\"The image is classified as a terrestrial ecosystem that is in {prediction}.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-devcon2024",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
